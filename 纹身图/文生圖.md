[[一丹一世界]]

remove noise in
- [[pixel space]]
- [[implicit place]]


### optimize
- 更大的模型容量
	SD XL only add a refiner( can be remove without affecting the fundamental outcome)
	the increase in params is mainly because of transformer blocks
- 更强的prompt following ability
	Dallie 3 recreate a 打标器, to precisely caption the outcome , and use them to retrain the model to make it more obedient
	More text encoder, more embedding dimension
 - 模型架构选择，UNetor Pure Transformer?
	 Although transformer unit all of the NLP missions, in terns of producing the pictures, it didn't have much strength, so we use u-net.
	However, Open AI Sora uses [[DiT ( Diffusion Transformer)]]

### 可控生成与图像的编辑
除了文本prompt以外，使用其他的控制条件引导扩散过程，例如风格，实体，姿态，边缘信息等等
![[Pasted image 20250205151444.png]]
##### [[空间控制]]
![[Pasted image 20250205151649.png]]

定制化生成
物体，风格，人脸

不可以使用统一架构来处理

- textual inversion
	Since dun train unet , only train the additional token embedding, the fidelity is bad
- DreamBooth
	Tune the whole model (unet)
	使用class-specific prior preservationloss壁免tuning的过程中catasprophy forgetting

IP - Adapter
zero shot

對偶cross attention
除了原先對文本，加上對圖片信息進行注意力計算
實現文本和圖片(物體，風格，人臉)共同控制擴散過程





定制化编辑

InstantID
- control net
	conditioning: 人臉的landmark
	用人臉編碼起提取特徵並送到(下
- IP-adapter


### speed up sampling
正向加噪:數據分布轉移至噪聲分布
反向去噪

SDE 
ODE 求解
	Consistency Model
	![[Pasted image 20250208163913.png]]
	(有多種)
	RF Rectified Flow
	![[Pasted image 20250208164100.png]]
	我们需要通过选择适当的epsilon(布長)来平衡这个速度和精度
	速度same，走直線



### 生態
##### 圖像
過去
latent diffusion 其實差強人意
![[Pasted image 20250208164534.png]]
VAE 編解碼(壓縮解壓縮)


input 轉成vector引導diffusion model
- token ID
- embedding
- transformer
- embedding (incuding synamic info)
- 跟去噪model交互(unet)

stable diffusion 
更大模型pretraining


BUT 以上都只能文本，缺精度。SO
- ControlNet
	點對點 
	邊緣
	結構控制
- LoRA
	lightweight tuning
	宏觀控制
- IP-Adapter
	介於兩者間
	風格控制
解決可控性問題

##### 影片
不連貫
AnimateDiff (開源，獨立)
Motion Modules穿插在unet裡，pipeline

[[SORA]]
快手 可靈
其他
rated by 
- 運動幅度(泛化能力，算力)
- 影片長度(誤差累積問題)
	stable video diffusion SVD model  圖生圖
	now solution:
	
	我们近期提出了一种视频生成模型的扩展训练方法，ExVideo，通过在长视频上继续训练，可以扩展现有视频生成模型到更长的时间尺度上，该模型可以在DiffSynth-Studio中使用
	也就是在他的在他原本的预训练过程结束之后进行后训练
![[Pasted image 20250208173651.png]]

主流架構
- ModelScopeT2V(時空分離)
	pros：
	沿用继承生成图模型
	还有时空分开后计算效率更高（不会平方爆炸）
	- [[attention 注意力機制]]
	
	cons：
	token embedding被分开，泛化能力/生成outcome may下降

- 时空一体的可能性？
在 SORA 的技术报告中他们提到他们把所有的这个叫做patch embedding 给做了扁平化


###### 未来

用时代的记忆来修补时代的记忆

3D渲染
	目前Nerf，高斯来建模

[[段忠杰大神的课后自由练习]]


### overall
![[Pasted image 20250209142657.png]]
diffusion 通过迭代生成更高quality

扩写后的提示词像咒语一样，具有一定的语法，常用的有
(xxx) 增加到 1.1倍权重
(xxx: 1.3) 增加到 1.3 倍权重
((xxx)) 叠加后增加到 1.21 倍权重

1-2




正向提示词用来描述想要出现的内容，负向提示词用来描述不想出现的内容。
![[Pasted image 20250209143505.png]]
（下面是负向）

CFG scale
	1就没有y的作用
	数字越小越接近训练集图片（越逼真）


![[Pasted image 20250209143750.png]]
去噪模型和 VAE 都会对图像进行上采样/下采样，把方形区域内的信息合并，所以模型生成的图像宽高只能是 16(或8，取决于模型结构)的倍数。

otherwise四舍五入到8/16的倍数，生成完图像后再resize，曲线救国
![[Pasted image 20250209144108.png]]SD3从以前的unet改为DIT

current BEST --
FLUX (Black Forrest Lab)



[[采样器]]

###### 图生图
![[Pasted image 20250209145122.png]]

重绘幅度denoising strength

局部重绘

高清修复（用超高清大图+小重绘幅度）
	otherwise一开始用2048生图易崩图



###### ControlNet
点对点控制
![[Pasted image 20250209150019.png]]

Control Type （由底模绑定）
需要一定适配的 Control Net Model
![[Pasted image 20250209150137.png]]

depth不会有纹理特征

lineat 动漫创作，方便上色

softedge 更细致，有过度

NormalMap 3D （逐被Depth取代）



[[ComfyUI]]