a German mathematician and physicist who introduced the concept of entropy around 1850, after he recognized the confusion between Carnot’s work and the conservation of energy.

Entropy is the measure of a system’s thermal energy per unit temperature that’s unavailable for doing work.
It’s also the measure of the disorder, or randomness, of a system.

Mathematically, if the entropy for a system is 0, then we have a reversible process with no change in entropy, like with a Carnot engine.
Any value over 0, and then the process is irreversible and gains entropy.

But, and this is very important, the entropy of the system’s surroundings and the universe would have to increase by an amount greater than or equal to the loss of entropy inside the system.
Simply put, our universe always tends toward disorder.
